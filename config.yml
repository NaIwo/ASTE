general:
  device: cuda # {cpu, cuda}
  logging-level: INFO
dataset:
  batch-size: 16
  effective-batch-size: 1 # gradient accumulation (after (batch-size * effective-batch-size) samples the gradient will be computed)
  number-of-polarities: 3
model:
  total-epochs: 120
  learning-rate: 0.0001
  early-stopping: 24 # num of epochs without improvement
  best-epoch-objective: TripletF1 # you can choose: loss and all of metrics
  bert:
    learning-rate: 0.00001
    source: bert-base-cased
  aggregators:
    endpoint:
      distance-embedding-dim: 3
  chunker:
    mode: hard # {soft, hard} # soft - use lambda factor during chunking (more info in text). hard - use hard chunking (chunk only in place where it is required)
    dice-loss-alpha: 0.0
    lambda-factor: 0.001
    loss-weight: 2.0
  selector:
    dice-loss-alpha: 0.5
    loss-weight: 1.0
  triplet-extractor:
    loss-weight: 3.0
encoder:
  bert:
    source: bert-base-cased
    embedding-dimension: 768