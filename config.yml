general:
  device: cuda # {cpu, cuda}
  logging-level: INFO
dataset:
  batch-size: 8
  effective-batch-size: 1 # gradient accumulation (after (batch-size * effective-batch-size) samples the gradient will be computed)
  number-of-polarities: 3
model:
  total-epochs: 120
  learning-rate: 0.0001
  early-stopping: 24 # num of epochs without improvement
  best-epoch-objective: SpanF1 # you can choose: loss and all of metrics
  transformer:
    learning-rate: 0.00001
    source: microsoft/deberta-base
  aggregators:
    endpoint:
      distance-embedding-dim: 3
  span_creator:
    loss-weight: 2.0
  selector:
    dice-loss-alpha: 0.5
    loss-weight: 1.0
    sigmoid-multiplication: 4. # In the training phase of the full model, increase the importance of the sigmoid input
                                # [sigmoid(value * neuron)] -
                                # more drastic incorrect span omitting
  triplet-extractor:
    loss-weight: 3.0
encoder:
  transformer:
    source: microsoft/deberta-base
    embedding-dimension: 768