general:
  device: cuda # {cpu, cuda}
  logging-level: INFO
dataset:
  batch-size: 4
  effective-batch-size: 4 # gradient accumulation (after (batch-size * effective-batch-size) samples the gradient will be computed)
model:
  total-epochs: 100
  learning-rate: 0.00001
  early-stopping: 10 # num of epochs without improvement
  best-epoch-objective: TripletF1 # you can choose: loss and all of metrics
  bert:
    source: bert-base-uncased
  aggregators:
    endpoint:
      distance-embedding-dim: 3
  chunker:
    dice-loss-alpha: 0.0
    lambda-factor: 0.001
    loss-weight: 1.0
  selector:
    dice-loss-alpha: 0.4
    loss-weight: 1.0
  triplet-extractor:
    dice-loss-alpha: 0.6
    loss-weight: 2.0
encoder:
  bert:
    source: bert-base-uncased
    embedding-dimension: 768